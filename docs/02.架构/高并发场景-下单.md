https://mp.weixin.qq.com/s/QW0Nd7O7yCWphuureEkElw

## 前言

高并发情况下无法直接处理业务，那么就出现了基于kafka实现高并发情况下的下单系统，中间一些细节处理

## 1. 顺序问题

### 1.1. 为什么要保证消息的顺序？

​		既然是走消息中间件`kafka`通信，下单系统发消息时将订单详细数据放在消息体，我们订单显示系统只要订阅`topic`，就能获取相关消息数据，然后处理自己的业务即可。

不过这套方案有个关键因素：**要保证消息的顺序**。

为什么呢？

订单有很多状态，比如：下单、支付、完成、撤销等，不可能`下单`的消息都没读取到，就先读取`支付`或`撤销`的消息吧，如果真的这样，数据不是会产生错乱？

好吧，看来保证消息顺序是有必要的。

### 1.2. 如何保证消息顺序？

我们都知道`kafka`的`topic`是无序的，但是一个`topic`包含多个`partition`，每个`partition`内部是有序的。

![图片](http://img.yluchao.cn/typora/67e6d2acf8ca7917ef4693f43fae86f9.png)

如此一来，思路就变得清晰了：只要保证生产者写消息时，按照一定的规则写到同一个`partition`，不同的消费者读不同的`partition`的消息，就能保证生产和消费者消息的顺序。

我们刚开始就是这么做的，同一个`商户编号`的消息写到同一个`partition`，`topic`中创建了`4`个`partition`，然后部署了`4`个消费者节点，构成`消费者组`，一个`partition`对应一个消费者节点。从理论上说，这套方案是能够保证消息顺序的。![图片](http://img.yluchao.cn/typora/e0c2d5268233a8c0cb3ec89aa2b8e649.png)

### 1.3 无重试机制导致消息丢失

最开始想法是：在消费者处理消息时，如果处理失败了，立马重试3-5次。但如果有些请求要第6次才能成功怎么办？不可能一直重试呀，这种同步重试机制，会阻塞其他商户订单消息的读取。

显然用上面的这种`同步重试机制`在出现异常的情况，会严重影响消息消费者的消费速度，降低它的吞吐量。

如此看来，我们不得不用`异步重试机制`了。

如果用异步重试机制，处理失败的消息就得保存到`重试表`下来。

但有个新问题立马出现：**只存一条消息如何保证顺序？**

存一条消息的确无法保证顺序，假如：”下单“消息失败了，还没来得及异步重试。此时，”支付“消息被消费了，它肯定是不能被正常消费的。

此时，”支付“消息该一直等着，每隔一段时间判断一次，它前面的消息都有没有被消费?

如果真的这么做，会出现两个问题：

1. ”支付“消息前面只有”下单“消息，这种情况比较简单。但如果某种类型的消息，前面有N多种消息，需要判断多少次呀，这种判断跟订单系统的耦合性太强了，相当于要把他们系统的逻辑搬一部分到我们系统。
2. 影响消费者的消费速度

这时有种更简单的方案浮出水面：消费者在处理消息时，先判断该`订单号`在`重试表`有没有数据，如果有则直接把当前消息保存到`重试表`。如果没有，则进行业务处理，如果出现异常，把该消息保存到`重试表`。

后来我们用`任务调度系统（elastic-job）`建立了`失败重试机制`，如果重试了`7`次后还是失败，则将该消息的状态标记为`失败`，报警通知开发人员。

## 2. 消息积压

随之而来的是消息的数量越来越大，导致消费者处理不过来，经常出现消息积压的情况。对商户的影响非常直观，划菜客户端上的订单和菜品可能半个小时后才能看到。

虽说，加`服务器节点`就能解决问题，但是按照公司为了省钱的惯例，要先做系统优化，所以我们开始了`消息积压`问题解决之旅。

### 2.1.  消息体过大

虽说`kafka`号称支持`百万级的TPS`，但从`producer`发送消息到`broker`需要一次网络`IO`，`broker`写数据到磁盘需要一次磁盘`IO`（写操作），`consumer`从`broker`获取消息先经过一次磁盘`IO`（读操作），再经过一次网络`IO`。

![图片](http://img.yluchao.cn/typora/20427bb8fe25790f1ef27914d3ed29b9.png)

一次简单的消息从生产到消费过程，需要经过`2次网络IO`和`2次磁盘IO`。如果消息体过大，势必会增加IO的耗时，进而影响kafka生产和消费的速度。消费者速度太慢的结果，就会出现消息积压情况。

除了上面的问题之外，`消息体过大`，还会浪费服务器的磁盘空间，稍不注意，可能会出现磁盘空间不足的情况。

**解决方案**

1. 订单系统发送的消息体只用包含：id和状态等关键信息。
2. 显示系统消费消息后，通过id调用订单系统的订单详情查询接口获取数据。
3. 后厨显示系统判断数据库中是否有该订单的数据，如果没有则入库，有则更新。

![图片](http://img.yluchao.cn/typora/6d7766a8cc3cbfab5423f12895668603.png)

### 2.2. 路由规则不合理

不是所有`partition`上的消息都有积压，而是只有一个或其中几个

![图片](http://img.yluchao.cn/typora/a452458c3237c1e4dca1da94fc07a2b7.png)

有几个商户的订单量特别大，刚好这几个商户被分到同一个`partition`，使得该`partition`的消息量比其他`partition`要多很多。

这时我们才意识到，发消息时按`商户编号`路由`partition`的规则不合理，可能会导致有些`partition`消息太多，消费者处理不过来，而有些`partition`却因为消息太少，消费者出现空闲的情况。

为了避免出现这种分配不均匀的情况，我们需要对发消息的路由规则做一下调整。

我们思考了一下，用订单号做路由相对更均匀，不会出现单个订单发消息次数特别多的情况。除非是遇到某个人一直加菜的情况，但是加菜是需要花钱的，所以其实同一个订单的消息数量并不多。

调整后按`订单号`路由到不同的`partition`，同一个订单号的消息，每次到发到同一个`partition`。

![图片](http://img.yluchao.cn/typora/4f5bd748d5c521d77e4696f34609469f.png)

调整后，消息积压的问题又有很长一段时间都没有再出现。我们的商户数量在这段时间，增长的非常快，越来越多了。

### 2.3 表过大

​		单表数据量过大，导致消息消费处理比较缓慢。

## 3. 重复消费

`kafka`消费消息时支持三种模式：

- at most onece模式 最多一次。保证每一条消息commit成功之后，再进行消费处理。消息可能会丢失，但不会重复。
- at least onece模式 至少一次。保证每一条消息处理成功之后，再进行commit。消息不会丢失，但可能会重复。
- exactly onece模式 精确传递一次。将offset作为唯一id与消息同时处理，并且保证处理的原子性。消息只会处理一次，不丢失也不会重复。但这种方式很难做到。

`kafka`默认的模式是`at least onece`，但这种模式可能会产生重复消费的问题，所以我们的业务逻辑必须做幂等设计。

而我们的业务场景保存数据时使用了`INSERT INTO ...ON DUPLICATE KEY UPDATE`语法，不存在时插入，存在时更新，是天然支持幂等性的。